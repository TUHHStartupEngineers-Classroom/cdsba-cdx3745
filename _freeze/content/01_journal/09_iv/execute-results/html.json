{
  "hash": "c701fe6178c20403059366b831039ac8",
  "result": {
    "markdown": "---\ntitle: \"Instrumental Variables\"\nauthor: \"Huzzaifa Khan\"\n---\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-1_8d71844d9db4516174f0dec9a00ce66e'}\n\n```{.r .cell-code}\nuser_df <- readRDS(\"C:/Users/huzai/OneDrive/Documents/GitHub/tes/cdsba-cdx3745/content/Causal_Data_Science_Data/Causal_Data_Science_Data/rand_enc.rds\")\n\nView(user_df)\n```\n:::\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-2_894d4619996858faeff237b0ac57a2c2'}\n\n```{.r .cell-code}\n#Assignment: 1\n\nlibrary(dagitty)\nlibrary(ggdag)\nlibrary(ggplot2)\n\napp_DAG <- dagify(\n  X ~ U,\n  Y ~ U,\n  Y ~ X,\n  X ~ Z,\n  coords = list(x = c(Z = 0, Y = 3, U = 2, X = 1),\n                y = c(Z = 0, Y = 0, U = 1, X = 0)),\n  labels = list(X = \"New Feature\",\n                Y = \"Time spent\", \n                Z = \"Confounders\",\n                IV = \"Popup_note\")\n)\n# Plot DAG\nggdag(app_DAG) +\n  theme_dag_gray() + # custom theme, can be left out\n  geom_dag_point(color = \"blue\") +\n  geom_dag_text(color = \"white\") +\n  geom_dag_edges(edge_color = \"black\") +\n  geom_dag_label_repel(aes(label = label))\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: Removed 1 rows containing missing values (`geom_label_repel()`).\n```\n:::\n\n::: {.cell-output-display}\n![](09_iv_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-3_429835fc408a48dbd0f14aaa7281d9b3'}\n\n```{.r .cell-code}\n#Assignment: 2\nmodel_full <- lm(time_spent ~ used_ftr, data = user_df)\nsummary(model_full)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = time_spent ~ used_ftr, data = user_df)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -20.4950  -3.5393   0.0158   3.5961  20.5051 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 18.86993    0.06955   271.3   <2e-16 ***\n#> used_ftr    10.82269    0.10888    99.4   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 5.351 on 9998 degrees of freedom\n#> Multiple R-squared:  0.497,\tAdjusted R-squared:  0.497 \n#> F-statistic:  9881 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-4_72302f1e95fb6379c63f536145c3faf4'}\n\n```{.r .cell-code}\n#Assignment: 3\n\n# Correlation matrix\ncor(user_df) %>% round(2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#>            rand_enc used_ftr time_spent\n#> rand_enc       1.00     0.20       0.13\n#> used_ftr       0.20     1.00       0.71\n#> time_spent     0.13     0.71       1.00\n```\n:::\n:::\n\n- As shown by the correlation matrix our instrument variable \"rand_enc\" is less correlated to our treatment variable used feature, that implies we have a weak instrument relevance. To test this assumption there might be some better procedure\n- Our instrument variable holds monotonically assumpotion as our correlation is positive\n-Our IV holds Exclusion restriction as the corr bw IV and Outcome \"time_spent\" is very low\n\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-5_45fdb4abc60ece23800d78368bba23b5'}\n\n```{.r .cell-code}\n#Assignment: 4\n\n# First stage\nfirst_stage <- lm(used_ftr ~ rand_enc, data = user_df)\nsummary(first_stage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = used_ftr ~ rand_enc, data = user_df)\n#> \n#> Residuals:\n#>     Min      1Q  Median      3Q     Max \n#> -0.5071 -0.3062 -0.3062  0.4929  0.6938 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept) 0.306164   0.006851   44.69   <2e-16 ***\n#> rand_enc    0.200940   0.009624   20.88   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 0.4811 on 9998 degrees of freedom\n#> Multiple R-squared:  0.04178,\tAdjusted R-squared:  0.04169 \n#> F-statistic:   436 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-6_11570286c59e0657afd797ea84016e6c'}\n\n```{.r .cell-code}\n# Second stage\nsecond_stage <- lm(user_df$time_spent ~ first_stage$fitted.values)\nsummary(second_stage)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = user_df$time_spent ~ first_stage$fitted.values)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -25.8757  -5.4714  -0.3263   5.3807  25.6541 \n#> \n#> Coefficients:\n#>                           Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)                19.3124     0.3129   61.72   <2e-16 ***\n#> first_stage$fitted.values   9.7382     0.7447   13.08   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 7.482 on 9998 degrees of freedom\n#> Multiple R-squared:  0.01681,\tAdjusted R-squared:  0.01672 \n#> F-statistic:   171 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell hash='09_iv_cache/html/unnamed-chunk-7_fd179e1d7b5005501225001b82ac4cf8'}\n\n```{.r .cell-code}\nlibrary(estimatr)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: package 'estimatr' was built under R version 4.3.2\n```\n:::\n\n```{.r .cell-code}\nmodel_iv <- iv_robust(time_spent ~ used_ftr | rand_enc, data = user_df)\nsummary(model_iv)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> iv_robust(formula = time_spent ~ used_ftr | rand_enc, data = user_df)\n#> \n#> Standard error type:  HC2 \n#> \n#> Coefficients:\n#>             Estimate Std. Error t value  Pr(>|t|) CI Lower CI Upper   DF\n#> (Intercept)   19.312     0.2248   85.89 0.000e+00   18.872    19.75 9998\n#> used_ftr       9.738     0.5353   18.19 8.716e-73    8.689    10.79 9998\n#> \n#> Multiple R-squared:  0.4921 ,\tAdjusted R-squared:  0.492 \n#> F-statistic:   331 on 1 and 9998 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nIn my opinion naive approach has better estimate as compared to the IV approach. as the std. error is quite less in naive. I can think of 2 reasons:\n\n1- there may exist a cofounding bw our IV and treatment variable.\n2- With IV approach it might be that we get a better LATE estimate as there can be good IV variable whiich has more relevance to the treatment.",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}
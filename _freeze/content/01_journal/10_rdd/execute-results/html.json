{
  "hash": "6505bf7b6e611ad5c09c40c43224a2ef",
  "result": {
    "markdown": "---\ntitle: \"Regression Discontinuity\"\nautor: \"Huzzaifa Khan\"\n---\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-1_361016d61c119564b2c274dbe40fd76f'}\n\n```{.r .cell-code}\ndf <- readRDS(\"C:/Users/huzai/OneDrive/Documents/GitHub/tes/cdsba-cdx3745/content/Causal_Data_Science_Data/Causal_Data_Science_Data/coupon.rds\")\n\nView(df)\n```\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-2_0b04a8140c794e7d13f5f9e64ad6646b'}\n\n```{.r .cell-code}\n# Define cut-off\nc0 <- 60\n\nlibrary(rddensity)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning: package 'rddensity' was built under R version 4.3.2\n```\n:::\n\n```{.r .cell-code}\nrddd <- rddensity(df$days_since_last, c = c0)\nsummary(rddd)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Manipulation testing using local polynomial density estimation.\n#> \n#> Number of obs =       5000\n#> Model =               unrestricted\n#> Kernel =              triangular\n#> BW method =           estimated\n#> VCE method =          jackknife\n#> \n#> c = 60                Left of c           Right of c          \n#> Number of obs         3854                1146                \n#> Eff. Number of obs    1486                734                 \n#> Order est. (p)        2                   2                   \n#> Order bias (q)        3                   3                   \n#> BW est. (h)           31.203              31.915              \n#> \n#> Method                T                   P > |T|             \n#> Robust                1.1559              0.2477\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in summary.CJMrddensity(rddd): There are repeated observations. Point\n#> estimates and standard errors have been adjusted. Use option massPoints=FALSE\n#> to suppress this feature.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> P-values of binomial tests (H0: p=0.5).\n#> \n#> Window Length / 2          <c     >=c    P>|T|\n#> 0.500                      20      20    1.0000\n#> 1.000                      31      35    0.7122\n#> 1.500                      44      47    0.8341\n#> 2.000                      70      68    0.9322\n#> 2.500                      92      89    0.8819\n#> 3.000                     110     105    0.7851\n#> 3.500                     123     118    0.7967\n#> 4.000                     135     129    0.7584\n#> 4.500                     148     142    0.7691\n#> 5.000                     164     159    0.8239\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-3_06b5c9faa0d31fd69bab9c53a98f480a'}\n\n```{.r .cell-code}\n# Assignment: 1\n\nlibrary(tidyverse)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n```\n:::\n\n```{.r .cell-code}\nhalf_BW <- c0 +c(-5/2,5/2)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(days_since_last >= half_BW [1] & days_since_last < c0)\ndf_bw_above <- df %>% filter(days_since_last >= c0 & days_since_last <= half_BW [2])\n\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 181   4\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-4_c79f6e6f7ace75feab7d4f12caa1fb48'}\n\n```{.r .cell-code}\n#Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above <- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 <- predict(model_bw_below, tibble(days_since_last = c0))\ny1 <- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate <- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"LATE: 7.36\"\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-5_35edd916470b1f47cbb832b539a49a21'}\n\n```{.r .cell-code}\n# [4] Estimation \n# Compute coefficients for specified bandwidth.\nlm_bw <- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#>     data = df_bw)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -10.9680  -2.2013   0.1676   2.1516   8.2567 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)               11.6612     0.5747  20.292  < 2e-16 ***\n#> days_since_last_centered   0.6883     0.3219   2.138   0.0339 *  \n#> couponTRUE                 7.1679     1.0172   7.047 3.87e-11 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.289 on 178 degrees of freedom\n#> Multiple R-squared:  0.6622,\tAdjusted R-squared:  0.6584 \n#> F-statistic: 174.5 on 2 and 178 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-6_d6e7de56975deb24eeb7335a05c1e597'}\n\n```{.r .cell-code}\n# Assignment: 2\n\ndbl_BW <- c0 + c(-10, 10)\n\n# Subsets below and above threshold in specified bandwidth\ndf_bw_below <- df %>% filter(days_since_last >= dbl_BW [1] & days_since_last < c0)\ndf_bw_above <- df %>% filter(days_since_last >= c0 & days_since_last <= dbl_BW [2])\n\ndf_bw <- bind_rows(df_bw_above, df_bw_below)\ndim(df_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] 629   4\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-7_21054e72ce95f7a2983c0c7722f81e2d'}\n\n```{.r .cell-code}\n#Local Average treatment effect (LATE) ----\n# Extract values for vertical lines to visualize local average treatment effect\nmodel_bw_below <- lm(purchase_after ~ days_since_last, df_bw_below)\nmodel_bw_above <- lm(purchase_after ~ days_since_last, df_bw_above)\n\ny0 <- predict(model_bw_below, tibble(days_since_last = c0))\ny1 <- predict(model_bw_above, tibble(days_since_last = c0))\n\nlate <- y1 - y0\nsprintf(\"LATE: %.2f\", late)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> [1] \"LATE: 9.51\"\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-8_358871e754b3337f85657e2789eab8d9'}\n\n```{.r .cell-code}\n# [4] Estimation \n# Compute coefficients for specified bandwidth.\nlm_bw <- lm(purchase_after ~ days_since_last_centered + coupon, df_bw)\nsummary(lm_bw)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Call:\n#> lm(formula = purchase_after ~ days_since_last_centered + coupon, \n#>     data = df_bw)\n#> \n#> Residuals:\n#>      Min       1Q   Median       3Q      Max \n#> -12.2718  -2.0858  -0.0003   2.0275  10.6749 \n#> \n#> Coefficients:\n#>                          Estimate Std. Error t value Pr(>|t|)    \n#> (Intercept)              10.61700    0.27386  38.767   <2e-16 ***\n#> days_since_last_centered  0.01413    0.04255   0.332     0.74    \n#> couponTRUE                9.51584    0.48628  19.569   <2e-16 ***\n#> ---\n#> Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#> \n#> Residual standard error: 3.115 on 626 degrees of freedom\n#> Multiple R-squared:  0.7052,\tAdjusted R-squared:  0.7042 \n#> F-statistic: 748.6 on 2 and 626 DF,  p-value: < 2.2e-16\n```\n:::\n:::\n\n\nwhen we have a half BW than the size of observation gets smaller to 181 and when we double the BW it gets double.  The estimate for our outcome increases with decreasing the BW and P-value is more better to reject null hypothesis and support alternative hypothesis.\n\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-9_0584b58f12505706bbccf1c59ac9687f'}\n\n```{.r .cell-code}\n# Assignment: 3\n\nship_df <- readRDS(\"C:/Users/huzai/OneDrive/Documents/GitHub/tes/cdsba-cdx3745/content/Causal_Data_Science_Data/Causal_Data_Science_Data/shipping.rds\")\n\nView(ship_df)\n```\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-10_337f6092335b559d75dc483ea68dba34'}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Create a histogram\nggplot(ship_df, aes(x = purchase_amount)) +\n  geom_histogram(binwidth = 4, fill = \"red\", color = \"blue\", alpha = 0.8) +\n  geom_vline(xintercept = 30, linetype = \"dashed\", color = \"yellow\", linewidth = 0.6) +\n  labs(title = \"histogram distribution of Purchase Amounts\",\n       x = \"Purchase Amount (€)\",\n       y = \"Num of Customers\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](10_rdd_files/figure-html/unnamed-chunk-10-1.png){width=672}\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-11_ff5dab72d0ff6add3c3279136744bf9f'}\n\n```{.r .cell-code}\ndist_dens <- rddensity(ship_df$purchase_amount, c = 30)\nsummary(dist_dens)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> Manipulation testing using local polynomial density estimation.\n#> \n#> Number of obs =       6666\n#> Model =               unrestricted\n#> Kernel =              triangular\n#> BW method =           estimated\n#> VCE method =          jackknife\n#> \n#> c = 30                Left of c           Right of c          \n#> Number of obs         3088                3578                \n#> Eff. Number of obs    2221                1955                \n#> Order est. (p)        2                   2                   \n#> Order bias (q)        3                   3                   \n#> BW est. (h)           22.909              20.394              \n#> \n#> Method                T                   P > |T|             \n#> Robust                5.9855              0\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\n#> Warning in summary.CJMrddensity(dist_dens): There are repeated observations.\n#> Point estimates and standard errors have been adjusted. Use option\n#> massPoints=FALSE to suppress this feature.\n```\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\n#> \n#> P-values of binomial tests (H0: p=0.5).\n#> \n#> Window Length / 2          <c     >=c    P>|T|\n#> 0.261                      20      26    0.4614\n#> 0.522                      41      65    0.0250\n#> 0.783                      62     107    0.0007\n#> 1.043                      81     136    0.0002\n#> 1.304                     100     169    0.0000\n#> 1.565                     114     196    0.0000\n#> 1.826                     132     227    0.0000\n#> 2.087                     156     263    0.0000\n#> 2.348                     173     298    0.0000\n#> 2.609                     191     331    0.0000\n```\n:::\n:::\n\n::: {.cell hash='10_rdd_cache/html/unnamed-chunk-12_3c580c65af2ff391f219351dc3a06317'}\n\n```{.r .cell-code}\ndist_plot <- rdplotdensity(dist_dens, ship_df$purchase_amount, plotN = 100)\n```\n\n::: {.cell-output-display}\n![](10_rdd_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\nIn my opinion RDD is not a well suited approach for this kind of treatment. As its seen in the Mccary denstiy test that the plot has no overlap of confidences around the cutoff point which is 30. \n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\r\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\r\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}